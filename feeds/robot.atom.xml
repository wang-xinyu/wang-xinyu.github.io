<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>BlogWX</title><link href="https://wang-xinyu.github.io/" rel="alternate"></link><link href="https://wang-xinyu.github.io/feeds/robot.atom.xml" rel="self"></link><id>https://wang-xinyu.github.io/</id><updated>2017-08-17T00:00:00+08:00</updated><entry><title>EKF SLAM-The Most Primitive SLAM</title><link href="https://wang-xinyu.github.io/ekf-slam-the-most-primitive-slam.html" rel="alternate"></link><updated>2017-08-17T00:00:00+08:00</updated><author><name>WangXinyu</name></author><id>tag:wang-xinyu.github.io,2017-08-17:ekf-slam-the-most-primitive-slam.html</id><summary type="html">&lt;h1&gt;EKF SLAM-The Most Primitive SLAM&lt;/h1&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In SLAM, the robot is given measurements $z_{1:t}$ and controls $u_{1:t}$, and acquires a map of its environment while simultaneously localizing itself relative to this map. From a probabilistic perspective, the online SLAM involves estimating the posterior over the current pose along with he map:&lt;/p&gt;
&lt;p&gt;$$
p(x_{t}, m | z_{1:t}, u_{1:t})
$$&lt;/p&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The maps in EKF SLAM are feature-based, they are composed of point landmarks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With known correspondences or data associations between observations and landmarks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the noises are Gaussian.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;State Vector&lt;/h2&gt;
&lt;p&gt;$$
y_t = (x,y,\theta,m_{1,x},m_{1,y},...,m_{n,x},m_{n,y})^T
$$&lt;/p&gt;
&lt;p&gt;The state vector includes robot pose and landmark positions. The dimension of this state vector is $2N+3$, where $N$ denotes the number of landmarks in the map. In robot localization problem, the state vector only contains robot pose, but for this EKF SLAM problem, we need to update the position of landmarks also. So now the posterior can be denoted as:&lt;/p&gt;
&lt;p&gt;$$
p(y_{t}| z_{1:t}, u_{1:t})
$$&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;——————————————&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Algorithm EKF_SLAM_known_correspondences($\mu_{t-1}$, $\Sigma_{t-1}$, $u_{t}$, $z_{t}$)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\bar \mu_t = g(u_t, \mu_{t-1})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\bar \Sigma_t = G_t \Sigma_{t-1} G_t^T+ R_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$K_t = \bar \Sigma_t H_t^T(H_t \bar \Sigma_t H_t^T + Q_t)^{-1}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;$\mu_t = \bar \mu_t + K_t(z_t - \hat z_t)$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Sigma_t = (I - K_t H_t) \bar \Sigma_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;return $\mu_t$, $\Sigma_t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;——————————————&lt;/p&gt;
&lt;p&gt;Basically, we are using EKF to estimate the belief of robot pose and landmark positions. The filter cycle is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State prediction, line 2 and 3, to predict the next state based on a motion model.&lt;/li&gt;
&lt;li&gt;Measurement prediction, for each observed landmark, we calculate its predicted measurement using the predicted robot pose and this landmark's position. We record the landmark's position the first time we see it. With this value, we can calculate $H_t$, $K_t$, $\hat z_t$ and $\Sigma_t$&lt;/li&gt;
&lt;li&gt;Measurement, after incorporating the measurement, we can calculate $\mu_t$&lt;/li&gt;
&lt;li&gt;Data association, we can skip this step, because the correspondences are known, but we need this step, if they are unknown.&lt;/li&gt;
&lt;li&gt;Update, update the belief.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;State Prediction&lt;/h2&gt;
&lt;p&gt;$
F_x = \begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp;\cdots &amp;amp; 0 \end{pmatrix}_{3 \times (2N+3)}
$&lt;/p&gt;
&lt;p&gt;$
y_t = y_{t-1} + F_x^T \begin{pmatrix} - \frac {v_t}{\omega_t}sin\mu_{t-1, \theta} + \frac {v_t}{\omega_t}sin(\mu_{t-1,\theta + \omega_t \Delta t})\\ \frac {v_t}{\omega_t}cos\mu_{t-1, \theta} - \frac {v_t}{\omega_t}cos(\mu_{t-1,\theta + \omega_t \Delta t}) \\ \omega_t \Delta t \end{pmatrix} + \mathcal{N} \left(0, F_x^T R_t F_x \right)
$&lt;/p&gt;
&lt;p&gt;As robot moves, the state changes according to the Velocity Motion Model. Because the motion only affects the robot pose and all landmarks remain where they are, only the first three elements in the update are non-zero.&lt;/p&gt;
&lt;p&gt;$
\bar \mu_t = \mu_{t-1} + F_x^T \begin{pmatrix} - \frac {v_t}{\omega_t}sin\mu_{t-1, \theta} + \frac {v_t}{\omega_t}sin(\mu_{t-1,\theta + \omega_t \Delta t})\\ \frac {v_t}{\omega_t}cos\mu_{t-1, \theta} - \frac {v_t}{\omega_t}cos(\mu_{t-1,\theta + \omega_t \Delta t}) \\ \omega_t \Delta t \end{pmatrix}
$&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Sebastian THRUN, Wolfram BURGARD, Dieter FOX. PROBABILISTIC ROBOTICS. 2005.&lt;/p&gt;
&lt;p&gt;Cyrill Stachniss. EKF SLAM, YouTube.com&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>The Derivation of Generic Particle Filter for Robot</title><link href="https://wang-xinyu.github.io/the-derivation-of-generic-particle-filter-for-robot.html" rel="alternate"></link><updated>2017-06-26T00:00:00+08:00</updated><author><name>WangXinyu</name></author><id>tag:wang-xinyu.github.io,2017-06-26:the-derivation-of-generic-particle-filter-for-robot.html</id><summary type="html">&lt;h1&gt;机器人的普通粒子滤波推导&lt;/h1&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;我这里所说的普通粒子滤波也叫SIR（Sampling Importance Resampling）粒子滤波。因为时至今日已产生了很多粒子滤波的变种，例如：regularized particle filter，Rao-Blackwellized particle filter等等，这些都使用了某种方法对基本的粒子滤波进行了优化。&lt;/p&gt;
&lt;p&gt;粒子滤波可以说是使用Monte Carlo方法的贝叶斯滤波的实现。这是因为SIS（Sequential Importance Sampling）是一种Monte Carlo方法，而粒子滤波、bootstap filter、condensation算法等，都可以基于SIS推导出来。&lt;/p&gt;
&lt;p&gt;Kalman滤波只能处理高斯分布，且状态转移方程和测量方程都必须线性。EKF是通过Taylor级数展开，把状态转移方程和测量方程线性化，仍假设噪声是高斯分布。而粒子滤波就可以处理非线性、任意分布的问题了。&lt;/p&gt;
&lt;h2&gt;模型&lt;/h2&gt;
&lt;p&gt;与Kalman滤波不同，粒子滤波不是估计$t$时刻状态向量的置信分布$bel(x_t)$，而是$0:t$时刻所有状态向量序列的置信分布$bel(x_{0:t})$，可以看作是一个轨迹。&lt;/p&gt;
&lt;p&gt;$$
bel(x_{0:t}) = p(x_{0:t}|z_{1:t},u_{1:t})
$$&lt;/p&gt;
&lt;p&gt;我们可以用一堆粒子来表示这个分布，每个粒子都有一个状态$x_{0:t}^i$和一个权重$w_t^i$，权重是归一化的，因此，$bel(x_{0:t})$可由如下的方程近似表示，$\delta(x)$是Dirac delta函数：&lt;/p&gt;
&lt;p&gt;$$
particle:\; \lbrace x_{0:t}^i, w_t^i \rbrace_{i=1}^N
$$&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^N w_t^i = 1
$$&lt;/p&gt;
&lt;p&gt;$$
bel(x_{0:t}) \approx \sum_{i=1}^N w_t^i \delta (x_{0:t} - x_{0:t}^i)
$$&lt;/p&gt;
&lt;h2&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;计算权重$w$，需要使用Importance Sampling。&lt;/p&gt;
&lt;p&gt;一般情况下，计算一个目标函数$f(x)$的期望，我们需要知道$x$的概率分布$p(x)$，然后积分求得期望，但实际中可能求积分比较困难，所以可以根据$p(x)$采样，然后用Monte Carlo方法近似计算：&lt;/p&gt;
&lt;p&gt;$$
E(f(x)) = \int f(x)p(x)dx \approx \frac 1 N \sum_{i=1}^N f(x_i)
$$&lt;/p&gt;
&lt;p&gt;$$
x_{0:N} \sim p(x)
$$&lt;/p&gt;
&lt;p&gt;但是，有时候我们不知道$p(x)$什么样，我们这里$p(x)$就是想求的target distribution是$bel(x_{0:t})$，所以没办法直接对它进行采样。这时就要用Importance Sampling了，引入一个新的分布$q(x)$，做如下变换，然后就可以根据$q(x)$采样，进而计算期望：&lt;/p&gt;
&lt;p&gt;$$
E(f(x)) = \int {f(x)p(x) \over q(x)} q(x) dx \approx \frac 1 N \sum_{i=1}^N f(x_i) {p(x_i) \over q(x_i)}
$$&lt;/p&gt;
&lt;p&gt;$$
x_{0:N} \sim q(x)
$$&lt;/p&gt;
&lt;p&gt;$$
Importance \, Weights: \; w_i = {p(x_i) \over q(x_i)}
$$&lt;/p&gt;
&lt;p&gt;那么回到我们的问题上，权重$w_t^i$如下，因为我们这里用权重表示概率，需要归一化，所以省略了归一化系数，用了一个正比符号。&lt;/p&gt;
&lt;p&gt;$$
w_t^i \propto {p(x_{0:t}^i|z_{1:t}, u_{1:t}) \over q(x_{0:t}^i|z_{1:t}, u_{1:t})}
$$&lt;/p&gt;
&lt;h2&gt;权重的递推公式&lt;/h2&gt;
&lt;p&gt;\begin{split}
p(x_{0:t}|z_{1:t}, u_{1:t}) &amp;amp;＝ {p(z_t|x_{0:t}, z_{1:t-1}, u_{1:t})p(x_t|x_{0:t-1}, z_{1:t-1}, u_{1:t})p(x_{0:t-1}, z_{1:t-1}, u_{1:t}) \over p(z_{1:t}, u_{1:t})} \; Bayes\\
&amp;amp;= {p(z_t|x_{t})p(x_t|x_{t-1}, u_{t})p(x_{0:t-1}, z_{1:t-1}, u_{1:t}) \over p(z_{1:t}, u_{1:t})} \; Markov假设\\
&amp;amp;= {p(z_t|x_{t})p(x_t|x_{t-1}, u_{t})p(x_{0:t-1}|z_{1:t-1}, u_{1:t}) \over p(z_t|z_{1:t-1}, u_{1:t})} \; Bayes\\
&amp;amp;= {p(z_t|x_{t})p(x_t|x_{t-1}, u_{t})p(x_{0:t-1}|z_{1:t-1}, u_{1:t-1}) \over p(z_t|z_{1:t-1}, u_{1:t})} \; 忽略u_t \\
&amp;amp;\propto p(z_t|x_{t})p(x_t|x_{t-1}, u_{t})p(x_{0:t-1}|z_{1:t-1}, u_{1:t-1})
\end{split}&lt;/p&gt;
&lt;p&gt;$$
q(x_{0:t}|z_{1:t}, u_{1:t}) ＝ q(x_t|x_{0:t-1}, z_{1:t}, u_{1:t})q(x_{0:t-1}|z_{1:t-1}, u_{1:t-1}) \; Bayes, 忽略z_t, u_t
$$&lt;/p&gt;
&lt;p&gt;\begin{split}
w_t^i &amp;amp;\propto {p(z_t|x_{t}^i)p(x_t^i|x_{t-1}^i, u_{t})p(x_{0:t-1}^i|z_{1:t-1}, u_{1:t-1}) \over q(x_t^i|x_{0:t-1}^i, z_{1:t}, u_{1:t})q(x_{0:t-1}^i|z_{1:t-1}, u_{1:t-1})}\\
&amp;amp;= w_{t-1}^i {p(z_t|x_{t}^i)p(x_t^i|x_{t-1}^i, u_{t}) \over q(x_t^i|x_{0:t-1}^i, z_{1:t}, u_{1:t})}
\end{split}&lt;/p&gt;
&lt;p&gt;如果根据Markov假设进一步做如下简化，则$t$时刻的状态估计至于$t-1$时刻的状态和当前的测量向量和控制向量有关，与$0:t-1$时刻的状态路径以及历史测量值都无关。&lt;/p&gt;
&lt;p&gt;$$
w_t^i \propto w_{t-1}^i {p(z_t|x_{t}^i)p(x_t^i|x_{t-1}^i, u_{t}) \over q(x_t^i|x_{t-1}^i, z_{t}, u_{t})}
$$&lt;/p&gt;
&lt;p&gt;因此我们可以近似认为路径估计等于当前状态估计：&lt;/p&gt;
&lt;p&gt;$$
bel(x_{0:t}) = p(x_{0:t}|z_{1:t},u_{1:t}) \approx p(x_{t}|z_{1:t},u_{1:t})
$$&lt;/p&gt;
&lt;p&gt;即：&lt;/p&gt;
&lt;p&gt;$$
bel(x_{t}) \approx \sum_{i=1}^N w_t^i \delta (x_{0:t} - x_{0:t}^i)
$$&lt;/p&gt;
&lt;h2&gt;选择Proposal Distribution&lt;/h2&gt;
&lt;p&gt;至此我们可以递归地计算权重了，但是权重的递推公式里面有一个proposal distribution－$q(x_t|x_{t-1}, z_{t}, u_{t})$还不知道是什么。&lt;/p&gt;
&lt;p&gt;$q(x_t|x_{t-1}, z_{t}, u_{t})$的最优解是$p(x_t|x_{t-1}, z_{t}, u_{t})$，为什么是最优我还没搞清楚-_-!&lt;/p&gt;
&lt;p&gt;我们通常会选择$p(x_t|x_{t-1}, u_{t})$作为$q(x_t|x_{t-1}, z_{t}, u_{t})$，如此一来，我们的权重递推公式可以进一步得到简化：&lt;/p&gt;
&lt;p&gt;$$
w_t^i \propto w_{t-1}^i p(z_t|x_{t}^i)
$$&lt;/p&gt;
&lt;h2&gt;Resampling&lt;/h2&gt;
&lt;p&gt;然后还有重要的一个步骤Resampling，Resampling是为了解决Degeneracy的问题。Degeneracy简单来说就是，经过若干次迭代后，权重都集中在了一个粒子的身上，其他粒子的权重都变得很小。&lt;/p&gt;
&lt;p&gt;Resampling就是对下面的离散分布进行重采样，用新的粒子集$\lbrace x_t^{i*}\rbrace_{i=1}^N$替换旧的粒子集$\lbrace x_t^i\rbrace_{i=1}^N$。采样时，旧的粒子是否保留的概率与权重$w_t^i$成正比。&lt;/p&gt;
&lt;p&gt;$$
bel(x_{t}) \approx \sum_{i=1}^N w_t^i \delta (x_{0:t} - x_{0:t}^i)
$$&lt;/p&gt;
&lt;p&gt;Resampling后每个粒子的权重被置为$\frac 1 N$。如果每一次迭代都进行Resampling的话（SIR粒子滤波就是这样做的），那我们的权重可以进一步简化为：&lt;/p&gt;
&lt;p&gt;$$
w_t^i \propto p(z_t|x_{t}^i)
$$&lt;/p&gt;
&lt;h2&gt;算法&lt;/h2&gt;
&lt;p&gt;——————————————&lt;/p&gt;
&lt;p&gt;Algorithm SIR Particle Filter($x_{t-1}$, $u_{t}$, $z_{t}$)&lt;/p&gt;
&lt;p&gt;for i = 1:N&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample $x_t^i$ from $p(x_t|x_{t-1}^i, u_{t})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_t^i = p(z_t|x_t^i)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;end for&lt;/p&gt;
&lt;p&gt;Normalize $\lbrace w_t^i\rbrace_{i=1}^N$&lt;/p&gt;
&lt;p&gt;Resampling:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample from $x_t^i$ with probability $\propto w_t^i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\lbrace w_t^i\rbrace_{i=1}^N = \frac 1 N$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;end for&lt;/p&gt;
&lt;p&gt;——————————————&lt;/p&gt;
&lt;h2&gt;扩展&lt;/h2&gt;
&lt;p&gt;粒子滤波是个很大的研究课题，基于SIS框架，可以利用很多优化方法得到粒子滤波的各种变种。例如：选择不同的proposal，使用不同的Resampling方法等等。&lt;/p&gt;
&lt;p&gt;粒子滤波有两个主要问题，一个是Degeneracy，可以通过逼近最优的proposal，或者Resampling来缓解。二是Resampling造成的sampling impoverishment。通过不同的方法优化这些问题，可产生很多粒子滤波的变种。针对具体的问题和应用场景，在传统粒子滤波的基础上优化某个点，也许会产生更好的效果。&lt;/p&gt;
&lt;h3&gt;参考文献&lt;/h3&gt;
&lt;p&gt;Sebastian THRUN, Wolfram BURGARD, Dieter FOX. PROBABILISTIC ROBOTICS. 2005.&lt;/p&gt;
&lt;p&gt;M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking. IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 2, FEBRUARY 2002.&lt;/p&gt;
&lt;p&gt;徐亦达. Particle Filter Part1-8, YouTube.com&lt;/p&gt;
&lt;h4&gt;版权声明：本文为作者原创，转载请注明出处&lt;a href="https://wang-xinyu.github.io"&gt;https://wang-xinyu.github.io&lt;/a&gt;&lt;/h4&gt;</summary><category term="Math"></category></entry><entry><title>The Derivation of Kalman Filter from Bayes Filter</title><link href="https://wang-xinyu.github.io/the-derivation-of-kalman-filter-from-bayes-filter.html" rel="alternate"></link><updated>2017-06-17T00:00:00+08:00</updated><author><name>WangXinyu</name></author><id>tag:wang-xinyu.github.io,2017-06-17:the-derivation-of-kalman-filter-from-bayes-filter.html</id><summary type="html">&lt;h1&gt;从贝叶斯滤波到卡尔曼滤波&lt;/h1&gt;
&lt;h2&gt;卡尔曼滤波的三个假设&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; 状态转移方程是线性的，噪声服从均值为$0$，协方差为$R_t$的多元高斯分布。因此，状态转移概率服从均值$A_t x_{t-1} + B_t u_t$，协方差为$R_t$的多元高斯分布。&lt;/p&gt;
&lt;p&gt;$$
\epsilon_t \sim N(0, R_t)
$$&lt;/p&gt;
&lt;p&gt;$$
x_t = A_t x_{t-1} + B_t u_t + \epsilon_t
$$&lt;/p&gt;
&lt;p&gt;$$
p(x_t|u_t, x_{t-1}) \sim N(A_t x_{t-1} + B_t u_t, R_t)
$$&lt;/p&gt;
&lt;p&gt;$$
p(x_t|u_t, x_{t-1}) = det(2 \pi R_t)^{- \frac 1 2}exp\lbrace {- \frac 1 2}(x_t-A_t x_{t-1} - B_t u_t)^T R_t^{-1}(x_t-A_t x_{t-1} - B_t u_t)\rbrace
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; 测量方程是线性的，噪声服从均值为$0$，协方差为$Q_t$的多元高斯分布。因此，测量概率服从均值$C_t x_t$，协方差为$Q_t$的多元高斯分布。&lt;/p&gt;
&lt;p&gt;$$
\delta_t \sim N(0, Q_t)
$$&lt;/p&gt;
&lt;p&gt;$$
z_t = C_t x_t + \delta_t
$$&lt;/p&gt;
&lt;p&gt;$$
p(z_t|x_t) \sim N(C_t x_t, Q_t)
$$&lt;/p&gt;
&lt;p&gt;$$
p(z_t|x_t) = det(2 \pi Q_t)^{- \frac 1 2}exp\lbrace {- \frac 1 2}(z_t-C_t x_t)^T Q_t^{-1}(z_t-C_t x_t)\rbrace
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; 初始置信$bel(x_0)$概率服从均值为$\mu_0$，协方差为$\Sigma_0$的多元高斯分布。&lt;/p&gt;
&lt;p&gt;$$
bel(x_0) = p(x_0) = det(2 \pi \Sigma_0)^{- \frac 1 2}exp\lbrace {- \frac 1 2}(x_0- \mu_0)^T \Sigma_0^{-1}(x_0- \mu_0)\rbrace
$$&lt;/p&gt;
&lt;h2&gt;递推公式&lt;/h2&gt;
&lt;p&gt;只要满足以上三个假设，即可证明$bel(x_t)$也满足高斯分布。即：&lt;/p&gt;
&lt;p&gt;$$
bel(x_t) \sim N(\mu_t, \Sigma_t)
$$
$$
\overline {bel}(x_t) \sim N(\overline \mu_t, \overline \Sigma_t)
$$&lt;/p&gt;
&lt;p&gt;根据贝叶斯滤波的递推公式和以上的假设，即可推导出卡尔曼滤波的递推公式。因为满足高斯分布，所以我们只需递推均值和协方差。&lt;/p&gt;
&lt;p&gt;\begin{split}
\overline \mu_t &amp;amp;= A_t \mu_{t-1} + B_t u_t\\
\overline \Sigma_t &amp;amp;= A_t \Sigma_{t-1} A_t^T+ R_t\\
K_t &amp;amp;= \overline \Sigma_t C_t^T(C_t \overline \Sigma_t C_t^T + Q_t)^{-1} \\
\mu_t &amp;amp;= \overline \mu_t + K_t(z_t - C_t \overline \mu_t) \\
\Sigma_t &amp;amp;= (I - K_t C_t) \overline \Sigma_t
\end{split}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$K_t$称为Kalman增益&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$I$是单位矩阵&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;简略推导&lt;/h2&gt;
&lt;p&gt;由贝叶斯滤波器可知$\overline {bel}(x_t)$的公式如下，在卡尔曼滤波中，$p(x_t|x_{t-1}, u_t)$和$bel(x_{t-1})$都服从高斯分布。&lt;/p&gt;
&lt;p&gt;$$
\overline {bel}(x_t) = \int p(x_t|x_{t-1}, u_t) \cdot bel(x_{t-1})dx_{t-1}
$$&lt;/p&gt;
&lt;p&gt;$$
p(x_t|x_{t-1}, u_t) \sim N(x_t; A_t x_{t-1} + B_t u_t, R_t)
$$&lt;/p&gt;
&lt;p&gt;$$
bel(x_{t-1}) \sim N(x_{t-1};\mu_{t-1}, \Sigma_{t-1})
$$&lt;/p&gt;
&lt;p&gt;这里$\overline {bel}(x_t)$可以看作两个高斯分布的卷积。省略中间步骤，最终推导出$\overline {bel}(x_t)$也是高斯分布，均值和协方差如下：&lt;/p&gt;
&lt;p&gt;\begin{split}
\overline \mu_t &amp;amp;= A_t \mu_{t-1} + B_t u_t\\
\overline \Sigma_t &amp;amp;= A_t \Sigma_{t-1} A_t^T+ R_t
\end{split}&lt;/p&gt;
&lt;p&gt;由贝叶斯滤波器可知$bel(x_t)$的公式如下，在卡尔曼滤波中，$p(z_t|x_t)$也服从高斯分布，上面已证明$\overline {bel}(x_t)$也服从高斯分布。&lt;/p&gt;
&lt;p&gt;$$
bel(x_t) = \eta \cdot p(z_t|x_{t}) \cdot \overline {bel}(x_t)
$$&lt;/p&gt;
&lt;p&gt;$$
p(z_t|x_t) \sim N(z_t; C_t x_t, Q_t)
$$&lt;/p&gt;
&lt;p&gt;$$
\overline {bel}(x_t) \sim N(x_t; \overline \mu_t, \overline \Sigma_t)
$$&lt;/p&gt;
&lt;p&gt;因此$bel(x_t)$可以看作两个高斯分布的乘积，结果也是高斯分布。省略中间推导，最终得到的均值和协方差如下：&lt;/p&gt;
&lt;p&gt;\begin{split}
K_t &amp;amp;= \overline \Sigma_t C_t^T(C_t \overline \Sigma_t C_t^T + Q_t)^{-1} \\
\mu_t &amp;amp;= \overline \mu_t + K_t(z_t - C_t \overline \mu_t) \\
\Sigma_t &amp;amp;= (I - K_t C_t) \overline \Sigma_t
\end{split}&lt;/p&gt;
&lt;h3&gt;参考文献&lt;/h3&gt;
&lt;p&gt;Sebastian THRUN, Wolfram BURGARD, Dieter FOX. PROBABILISTIC ROBOTICS. 2005.&lt;/p&gt;
&lt;p&gt;概率机器人的贝叶斯滤波器. 我的博文，&lt;a href="https://wang-xinyu.github.io/bayes-filter-for-robot.html"&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;版权声明：本文为作者原创，转载请注明出处&lt;a href="https://wang-xinyu.github.io"&gt;https://wang-xinyu.github.io&lt;/a&gt;&lt;/h4&gt;</summary><category term="Math"></category></entry><entry><title>Bayes Filter for Robot</title><link href="https://wang-xinyu.github.io/bayes-filter-for-robot.html" rel="alternate"></link><updated>2017-06-13T00:00:00+08:00</updated><author><name>WangXinyu</name></author><id>tag:wang-xinyu.github.io,2017-06-13:bayes-filter-for-robot.html</id><summary type="html">&lt;h1&gt;概率机器人的贝叶斯滤波器&lt;/h1&gt;
&lt;h2&gt;模型&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$x_t$表示$t$时刻机器人的状态向量&lt;/li&gt;
&lt;li&gt;$z_t$表示$t$时刻的测量向量&lt;/li&gt;
&lt;li&gt;$u_t$表示$t$时刻的控制向量&lt;/li&gt;
&lt;li&gt;$bel(x_t)$表示状态向量的置信分布（Belief Distributions），根据它我们就可以确定$t$时刻机器人的状态，贝叶斯滤波器最终的输出结果就是它。它的定义如下，即在已知1到$t$时刻的测量向量和控制向量的条件下，机器人处于状态$x_t$的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
bel(x_t) = p(x_t|z_{1:t},u_{1:t})
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\overline {bel}(x_t)$是计算$bel(x_t)$的一个中间值，称为预测（Prediction）。它的定义如下，即在得到$t$时刻的测量向量$z_t$之前获取的置信概率，因此称为预测。而得到$z_t$后，由$\overline {bel}(x_t)$计算$bel(x_t)$的过程称为纠正或更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\overline {bel}(x_t) = p(x_t|z_{1:t-1},u_{1:t})
$$&lt;/p&gt;
&lt;h2&gt;递推公式&lt;/h2&gt;
&lt;p&gt;先给出贝叶斯滤波器的递推公式，然后证明。&lt;/p&gt;
&lt;p&gt;$$
\overline {bel}(x_t) = \int p(x_t|u_{t},x_{t-1}) \cdot bel(x_{t-1})dx_{t-1}
$$&lt;/p&gt;
&lt;p&gt;$$
bel(x_t) = \eta \cdot p(z_t|x_{t}) \cdot \overline {bel}(x_t)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$p(x_t|u_{t},x_{t-1})$是机器人的状态转移概率&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p(z_t|x_{t})$是测量概率&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\eta$是概率归一化常量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据这个公式我们可以看出来，只要知道状态转移概率分布，测量概率以及前一时刻的置信概率，就可以递归求得当前机器人状态的置信概率。&lt;/p&gt;
&lt;h2&gt;证明&lt;/h2&gt;
&lt;p&gt;\begin{split}
bel(x_t) &amp;amp;= p(x_t|z_{1:t},u_{1:t})\\
&amp;amp;= {p(x_t,z_{1:t},u_{1:t}) \over p(z_{1:t},u_{1:t})}条件概率\\
&amp;amp;= {p(z_t|x_t, z_{1:t-1},u_{1:t}) \cdot p(x_t, z_{1:t-1},u_{1:t}) \over p(z_{1:t},u_{1:t})}条件概率\\
&amp;amp;= {p(z_t|x_t, z_{1:t-1},u_{1:t}) \cdot p(x_t|z_{1:t-1},u_{1:t}) \cdot p(z_{1:t-1},u_{1:t}) \over p(z_t|z_{1:t-1},u_{1:t}) \cdot p(z_{1:t-1},u_{1:t})} 条件概率\\
&amp;amp;= {p(z_t|x_t, z_{1:t-1},u_{1:t}) \cdot p(x_t|z_{1:t-1},u_{1:t}) \over p(z_t|z_{1:t-1},u_{1:t})} 约分\\
&amp;amp;= {p(z_t|x_t) \cdot p(x_t|z_{1:t-1},u_{1:t}) \over p(z_t|z_{1:t-1},u_{1:t})} Markov假设\\
&amp;amp;= \eta \cdot p(z_t|x_t) \cdot \overline {bel}(x_t) 替换
\end{split}&lt;/p&gt;
&lt;p&gt;\begin{split}
\overline {bel}(x_t) &amp;amp;= p(x_t|z_{1:t-1},u_{1:t})\\
&amp;amp;= \int p(x_t|x_{t-1}, z_{1:t-1},u_{1:t}) \cdot p(x_{t-1}|z_{1:t-1},u_{1:t})dx_{t-1} 用全概率计算条件概率\\
&amp;amp;= \int p(x_t|x_{t-1}, u_{t}) \cdot p(x_{t-1}|z_{1:t-1},u_{1:t-1})dx_{t-1} Markov假设并去掉u_t\\
&amp;amp;= \int p(x_t|x_{t-1}, u_{t}) \cdot bel(x_{t-1})dx_{t-1} 替换
\end{split}&lt;/p&gt;
&lt;p&gt;这里的$\eta = {p(z_t|z_{1:t-1},u_{1:t})}^{-1}$，可以看作概率的归一化因子。因为它等于分子中两个PDF的乘积的积分：&lt;/p&gt;
&lt;p&gt;\begin{split}
p(z_t|z_{1:t-1},u_{1:t}) &amp;amp;＝ \int p(z_t|x_t, z_{1:t-1},u_{1:t}) \cdot p(x_t|z_{1:t-1},u_{1:t})dx_t用全概率计算条件概率\\
&amp;amp;= \int p(z_t|x_t) \cdot p(x_t|z_{1:t-1},u_{1:t})dx_t \; Markov假设
\end{split}&lt;/p&gt;
&lt;p&gt;Markov假设当前状态是完整的，若已知当前状态$x_t$，则$t$时刻及其之前的测量$z_{1:t}$和控制向量$u_{1:t}$对未来状态没有影响。&lt;/p&gt;
&lt;h3&gt;参考文献&lt;/h3&gt;
&lt;p&gt;Sebastian THRUN, Wolfram BURGARD, Dieter FOX. PROBABILISTIC ROBOTICS. 2005.&lt;/p&gt;
&lt;p&gt;M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking. IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 50, NO. 2, FEBRUARY 2002.&lt;/p&gt;
&lt;p&gt;使用全概率公式计算条件概率. 我的博文，&lt;a href="https://wang-xinyu.github.io/total-probability-applied-to-conditional-probability.html"&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;版权声明：本文为作者原创，转载请注明出处&lt;a href="https://wang-xinyu.github.io"&gt;https://wang-xinyu.github.io&lt;/a&gt;&lt;/h4&gt;</summary><category term="Math"></category></entry><entry><title>Getting Started with ROS Navigation Stack</title><link href="https://wang-xinyu.github.io/getting-started-with-ros-navigation-stack.html" rel="alternate"></link><updated>2016-08-19T00:00:00+08:00</updated><author><name>WangXinyu</name></author><id>tag:wang-xinyu.github.io,2016-08-19:getting-started-with-ros-navigation-stack.html</id><summary type="html">&lt;h1&gt;Getting Started with ROS Navigation Stack&lt;/h1&gt;
&lt;p&gt;ROS is a open-source framework for robot software development. Personally, I think ROS is the future of Robot. In this article, I'll introduce how to work with ROS Navigation Stack from the beginning.&lt;/p&gt;
&lt;h2&gt;1. Install ROS&lt;/h2&gt;
&lt;p&gt;ROS is not a standalone OS, but based on Ubuntu or other Debian distributions. So first we need to install Ubuntu, or other Debian if you like, but it is recommanded to choose Ubuntu for beginners.&lt;/p&gt;
&lt;p&gt;At the time of my writing this article, the latest ROS distribution is Kinetic, which &lt;strong&gt;ONLY&lt;/strong&gt; supports Wily (Ubuntu 15.10), Xenial (Ubuntu 16.04) and Jessie (Debian 8) for debian packages. So my choice is &lt;code&gt;Xenial + Kinetic&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://wiki.ros.org/kinetic/Installation/Ubuntu"&gt;ROS Kinetic Install Guide&lt;/a&gt; is very helpful. So I just make a brief summary here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup the repositories, sources.list and apt-key. So that you can install ROS packages through &lt;code&gt;apt-get install&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Install the full package &lt;code&gt;ros-kinetic-desktop-full&lt;/code&gt; if you are using a x86 PC. For ARM, such as Raspberry Pi, you may have to install ROS-base, and then install other individual packages.&lt;/li&gt;
&lt;li&gt;Setup your shell environment. Basically, you need to append &lt;code&gt;source /opt/ros/kinetic/setup.bash&lt;/code&gt; to your &lt;code&gt;~/.bashrc&lt;/code&gt;. After creating your own workspaces, you have to setup extra environments.&lt;/li&gt;
&lt;li&gt;Besides, there are some settings not that important, you may check the Kinetic Install Guide for details. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. Create a ROS Workspace&lt;/h2&gt;
&lt;p&gt;There are two ways to create ROS workspace, i.e. &lt;code&gt;catkin&lt;/code&gt; and &lt;code&gt;rosbuild&lt;/code&gt;. I think &lt;code&gt;catkin&lt;/code&gt; is much more easier.&lt;/p&gt;
&lt;p&gt;Actually you may follow the &lt;a href="http://wiki.ros.org/ROS/Tutorials"&gt;ROS beginner tutorials&lt;/a&gt; to learn how to create a catkin workspace, and you have to understand ROS packages, nodes, topics, services, parameters, roslaunch, messages, publisher, subscriber and other ROS basics.&lt;/p&gt;
&lt;h2&gt;3. Get your robot run with RViz&lt;/h2&gt;
&lt;p&gt;Now we are ready to drive the robot with ROS. Before using ROS, I think you should already have a driver program which can send movement command from PC to your robot and receive odometry and other robot state info. And then you need a bridge program to connect ROS with your driver.&lt;/p&gt;
&lt;p&gt;My robot is P3DX, the &lt;code&gt;libaria&lt;/code&gt; can communicate with robot, controll it and get its state info. And &lt;code&gt;RosAria&lt;/code&gt; is the bridge between ROS and &lt;code&gt;libaria&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RosAria&lt;/code&gt; subscribes to the &lt;code&gt;cmd_vel&lt;/code&gt; topic, so that I can make P3DX run by publishing &lt;code&gt;geometry_msgs/Twist&lt;/code&gt; message to it.&lt;/p&gt;
&lt;p&gt;And &lt;code&gt;RosAria&lt;/code&gt; publishes to the topic &lt;code&gt;pose&lt;/code&gt;, &lt;code&gt;sonar&lt;/code&gt;, &lt;code&gt;bumper_state&lt;/code&gt;, &lt;code&gt;motors_state&lt;/code&gt;, etc. I can get info from these topics.&lt;/p&gt;
&lt;p&gt;Then I want to display my robot on the RViz and teleop it. The &lt;a href="https://github.com/MobileRobots/amr-ros-config"&gt;amr-ros-config&lt;/a&gt; package contains &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.rviz&lt;/code&gt; and &lt;code&gt;launch&lt;/code&gt; files, with all these stuff, I can display the P3DX model on the RViz and teleop it.&lt;/p&gt;
&lt;h2&gt;4. Setup ROS Navigation Stack&lt;/h2&gt;
&lt;p&gt;&lt;img alt="ros_nav_stack.png" src="/images/nav_stack.png" /&gt;&lt;/p&gt;
&lt;p&gt;The blue components are what we need to provide. Fortunately, &lt;code&gt;RosAria&lt;/code&gt; has done most of the work. &lt;code&gt;odometry source&lt;/code&gt;, &lt;code&gt;base controller&lt;/code&gt; and &lt;code&gt;sensor sources&lt;/code&gt; are all fully provided by &lt;code&gt;RosAria&lt;/code&gt;. For &lt;code&gt;sensor transforms&lt;/code&gt;, &lt;code&gt;RosAria&lt;/code&gt; provides the tf &lt;code&gt;odom-&amp;gt;base_link&lt;/code&gt;, other sensor frames transform should be provided by us. I didn't use laser, I just used sonar, so I just provided the tf &lt;code&gt;base_link-&amp;gt;sonar_frame&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The gray components are optional components that are already implemented. Here I didn't use &lt;code&gt;amcl&lt;/code&gt;, I used &lt;code&gt;map_server&lt;/code&gt; with a existing map picture. So I need a YAML file to describe &lt;code&gt;map_server&lt;/code&gt; parameters.&lt;/p&gt;
&lt;p&gt;The white components are required components that are already implemented. What we have to provide is just four configuration files, i.e. &lt;code&gt;costmap_common_params.yaml&lt;/code&gt;, &lt;code&gt;global_costmap_params.yaml&lt;/code&gt;, &lt;code&gt;local_costmap_params.yaml&lt;/code&gt; and &lt;code&gt;base_local_planner_params.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, a launch file is needed to launch the ROS navigation stack. The launch sequence is like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RosAria + RViz&lt;/li&gt;
&lt;li&gt;map_server with map YAML file&lt;/li&gt;
&lt;li&gt;move_base with those four configuration files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK, now set a goal from RViz, there will be a path and local cosmap, then the robot will go down that path until reaching that goal.&lt;/p&gt;</summary><category term="ROS"></category></entry></feed>